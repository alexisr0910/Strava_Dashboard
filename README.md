# ðŸƒâ€â™‚ï¸ Strava Data Analytics Pipeline

Ce projet implÃ©mente un pipeline ELT (Extract, Load, Transform) complet pour automatiser la rÃ©cupÃ©ration de mes donnÃ©es Strava et les transformer en donnÃ©es exploitables.

###  [Consulter le Dashboard Power BI](https://app.powerbi.com/view?r=eyJrIjoiZGI0ZDRmZWUtMjQ5Zi00ZjU1LTllMzQtZjU2ZTYyNDczOTBhIiwidCI6IjI3YTU4YjYzLTg2ODQtNDBmNy1iNzM3LWM5YTUzNGU2NTc0NSJ9)

##  Choix Technologiques & Architecture

## Phase 1 : RÃ©cupÃ©rer des donnÃ©es (Extract & Load)
* **Source** : API Strava (OAuth2).
* **Outil de transport** : **Airbyte Open Source** dÃ©ployÃ© sur Docker.

Airbyte est une plateforme d'intÃ©gration de donnÃ©es qui aide Ã  rÃ©pliquer et Ã  consolider facilement des donnÃ©es provenant de diffÃ©rentes sources (bases de donnÃ©es, API, applications SaaS). J'ai choisi cet outil car il est open source et propose un connecteur dÃ©jÃ  prÃªt pour l'API Strava. C'est une solution reconnue qui rÃ©pondait exactement Ã  mon besoin technique.

####  Installation (Airbyte)
Lors de l'installation, j'ai rencontrÃ© des difficultÃ©s liÃ©es Ã  la puissance de calcul de mon Mac. Pour optimiser l'utilisation de la RAM sur MacOS, j'ai utilisÃ© le mode spÃ©cifique de consommation rÃ©duite `--low-resource-mode`.

**Commandes de gestion :**
* **DÃ©marrer** : `abctl local install --low-resource-mode --insecure-cookies`
* **Stopper** : `abctl local uninstall`

####  Configuration de la Source (Strava API)
* **Connexion** : Utilisation du protocole **OAuth2** en suivant la documentation officielle d'Airbyte.
* **Historique** : ParamÃ©trage de la date de dÃ©but au **1er janvier 2015** afin d'importer l'intÃ©gralitÃ© de mon historique sportif.
* **SÃ©curitÃ©** : Les identifiants sensibles (`Client ID`, `Client Secret`) sont gÃ©rÃ©s uniquement dans l'interface locale d'Airbyte.

#### Configuration de la Destination (Google BigQuery)
Pour le stockage et l'analyse, j'ai choisi **Google BigQuery** comme Data Warehouse Cloud pour sa capacitÃ© Ã  gÃ©rer de gros volumes et sa facilitÃ© de connexion aux outils de visualisation.

* **Projet GCP (Google Cloud Platform)** : CrÃ©ation du projet `dashboardstrava1`.
* **SÃ©curitÃ©** : CrÃ©ation d'un **compte de service** spÃ©cifique pour isoler les accÃ¨s.
* **Droits d'accÃ¨s** :
    * `Administrateur BigQuery` : Pour permettre Ã  l'outil d'Ã©crire les donnÃ©es.
    * `Administrateur Storage` : Sert de zone tampon pour fluidifier l'importation des gros volumes.
* **Dataset** : DonnÃ©es stockÃ©es dans `strava_raw` (Localisation : EU).

![img.png](images/Airbyte_connexion.png)

##  ParamÃ©trage du flux (Sync Mode)

J'ai configurÃ© deux modes diffÃ©rents dans Airbyte pour optimiser le pipeline :

* **ActivitÃ©s (`Incremental | Append + Deduped`)** :
  Ce mode permet de ne rÃ©cupÃ©rer que les nouvelles activitÃ©s sportives. Airbyte utilise l'identifiant unique (`id`) pour Ã©viter les doublons. Cela permet un chargement plus rapide et rÃ©duit la consommation de ressources.

* **Statistiques (`Full Refresh | Overwrite`)** :
  Pour mes records personnels et totaux globaux, j'ai choisi de remplacer intÃ©gralement les donnÃ©es Ã  chaque passage. Ces informations n'ayant pas d'ID unique, ce mode est nÃ©cessaire pour garantir des donnÃ©es toujours Ã  jour.

![img_1.png](images/Airbyte_data.png)
---

## Phase 2 : Transformation (Couche ODS)

L'objectif de cette Ã©tape est de transformer les donnÃ©es brutes (`strava_raw`) pour crÃ©er un dataset ODS (`strava_ods`) propre et structurÃ©.

### Objectifs de la couche ODS
* **Nettoyage** : Passage d'un format JSON Ã  une structure SQL exploitable.
* **Extraction des donnÃ©es** : SÃ©lection des seules donnÃ©es jugÃ©es pertinentes (distance, vitesse, dÃ©nivelÃ©, kudos, etc.).
* **DonnÃ©es sensibles** : Suppression dÃ©finitive des coordonnÃ©es GPS et des identifiants personnels dÃ¨s cette Ã©tape pour ne pas les stocker dans le reste du projet.

### Points techniques de la mise en place
* **VÃ©rification du typage** : Utilisation d'une table de test (`test_format`) pour confirmer que BigQuery et Airbyte interprÃ¨tent correctement les types numÃ©riques (`NUMERIC`, `INTEGER`).
* **Traitement du JSON** : Extraction des donnÃ©es imbriquÃ©es pour la table `athlete_stats`. J'ai utilisÃ© la fonction **`CAST`** pour transformer les donnÃ©es textuelles issues du JSON en formats numÃ©riques, ce qui permet de rÃ©aliser des calculs par la suite.

> **Pour consulter le dÃ©tail du mapping, les justifications de filtrage et les Ã©chantillons de donnÃ©es, voir : [Documentation dÃ©taillÃ©e ODS](./docs/ODS/README.md)**
---

---

## Phase 3 : ModÃ©lisation (Couche DWH)

L'objectif de cette derniÃ¨re Ã©tape de transformation est de passer d'une structure de donnÃ©es "plate" Ã  un **modÃ¨le dimensionnel (SchÃ©ma en Ã‰toile)** dans le dataset `strava_dwh`. Ce modÃ¨le est optimisÃ© pour les performances et la clartÃ© des analyses dans Power BI.

### Architecture du modÃ¨le en Ã©toile
Pour ce projet, j'ai structurÃ© les donnÃ©es autour d'une table de faits centrale et de plusieurs dimensions :

* **Table de Faits (`fct_activites`)** : Centralise toutes les mesures granulaires (distance, temps, vitesse).
* **Dimensions (`dim_calendar`, `dim_moment_journee`)** : Fournissent le contexte temporel et horaire pour filtrer les donnÃ©es.
* **Table Snapshot (`fct_global_stats`)** : Stocke les totaux historiques depuis 2015 pour servir de rÃ©fÃ©rentiel de vÃ©ritÃ©.

### Logique de transformation et calculs mÃ©tiers
Plusieurs transformations ont Ã©tÃ© opÃ©rÃ©es pour normaliser les indicateurs de performance :

1.  **Standardisation des unitÃ©s** : Conversion des donnÃ©es brutes en unitÃ©s lisibles : mÃ¨tres vers **kilomÃ¨tres**, m/s vers **km/h**, et secondes vers **minutes**.
2.  **Calcul de l'allure ** : CrÃ©ation de la mÃ©trique `allure_min_km`. C'est l'indicateur principal pour la course Ã  pied, calculÃ© via `SAFE_DIVIDE` pour garantir la stabilitÃ© du pipeline.
3.  **Choix du format numÃ©rique** : Les durÃ©es sont stockÃ©es en format **dÃ©cimal** (`FLOAT64`) et non en format horaire. Ce choix technique permet Ã  Power BI de rÃ©aliser des calculs mathÃ©matiques (moyennes, sommes) avant le formatage visuel final.
4.  **Localisation (FranÃ§ais)** : Contrairement aux rÃ©glages par dÃ©faut de BigQuery (Anglais), j'ai intÃ©grÃ© la traduction des jours et des mois directement en SQL via des instructions `CASE`. Cela permet de livrer un dataset "prÃªt Ã  l'emploi" pour la visualisation.

### Optimisation pour la visualisation
* **ClÃ© primaire** : CrÃ©ation d'un `date_id` (format `YYYYMMDD`) pour lier les activitÃ©s au calendrier.
* **Gestion du tri** : Ajout d'une colonne `ordre_tri` dans la dimension pour forcer Power BI Ã  afficher les moments de la journÃ©e (Matin, Midi, Soir) chronologiquement plutÃ´t qu'alphabÃ©tiquement.

> **Pour consulter le dÃ©tail de la structure du modÃ¨le, les formules SQL et les choix de modÃ©lisation, voir : [Documentation dÃ©taillÃ©e DWH](./docs/DWH/README.md)**
---

## Pistes d'amÃ©lioration

Ce projet constitue une **PoC (Proof of Concept)** solide qui dÃ©montre la viabilitÃ© du flux. Le pipeline est actuellement **semi-automatique** car il dÃ©pend d'un environnement local, mais plusieurs axes permettraient de le passer au niveau industriel :

### 1. Automatisation
Actuellement, le pipeline dÃ©pend d'Airbyte tournant sur mon Mac (Docker).
* **AmÃ©lioration** : DÃ©ployer Airbyte sur une instance et utiliser les **Scheduled Queries** de BigQuery. Cela permettrait une synchronisation et une transformation des donnÃ©es totalement autonomes durant la nuit, sans dÃ©pendance matÃ©rielle.

### 2. Monitoring et Alerting
Le suivi du pipeline nÃ©cessite aujourd'hui une vÃ©rification visuelle aprÃ¨s chaque exÃ©cution.
* **AmÃ©lioration** : Mettre en place un systÃ¨me de notifications (Email) pour Ãªtre alertÃ© en cas d'Ã©chec de la synchronisation ou d'une erreur SQL.
* **RÃ©fÃ©rence professionnelle** : Cette mÃ©thodologie est celle que j'applique dans le cadre de mon alternance au CHU. Chaque script de traitement gÃ©nÃ¨re un fichier de log dÃ©taillÃ©. Ces fichiers sont ensuite parcourus par un automate qui remonte par mail un rapport d'Ã©tat chaque matin, permettant de valider le bon dÃ©roulement des flux nocturnes ou d'intervenir rapidement en cas d'anomalie.

### 3. Orchestration
Les transformations SQL sont dÃ©clenchÃ©es manuellement de maniÃ¨re indÃ©pendante.
* **AmÃ©lioration** : Utiliser un orchestrateur pour gÃ©rer les dÃ©pendances (ex: ne pas lancer le DWH si l'ODS a Ã©chouÃ©).

---
### Power BI
---

# Phase 4 : Conception du Dashboard (Power BI)

![img.png](images/Mode_nuit.png)

Cette derniÃ¨re Ã©tape permet de mettre en image tout le travail effectuÃ© en amont sur BigQuery pour transformer la donnÃ©e brute en outil d'analyse.

### Pourquoi Power BI ?
Le choix de Power BI s'est imposÃ© naturellement car j'ai pu aborder cet outil lors de mes cours en **Master MIAGE**. Ce projet Ã©tait l'occasion parfaite de le reprendre en main sur un thÃ¨me personnel comme Strava. De plus, sa capacitÃ© Ã  se connecter nativement Ã  **Google BigQuery** Ã©tait un avantage majeur pour garantir un flux de donnÃ©es fluide et performant.

### Apprentissage et prise en main
MÃªme si j'avais des bases, j'ai souhaitÃ© pousser l'outil plus loin pour ce projet. J'ai appris Ã  utiliser des fonctions avancÃ©es Ã  l'aide de l'IA et de vidÃ©os YouTube spÃ©cialisÃ©es. Cela m'a permis d'intÃ©grer des spÃ©cificitÃ©s prÃ©cises pour amÃ©liorer l'expÃ©rience utilisateur :

* **Design Dynamique (Mode Jour/Nuit)** : CrÃ©ation d'un switch intelligent via des signets (bookmarks) et des mesures DAX pour adapter l'interface Ã  l'environnement de consultation.
* **Mesures Dynamiques** : Mise en place de sÃ©lecteurs permettant de basculer instantanÃ©ment l'affichage entre les **valeurs rÃ©elles** et les **pourcentages**, offrant ainsi deux niveaux de lecture sur un mÃªme visuel.
* **EsthÃ©tique** : Travail approfondi sur le design (ombres portÃ©es, contrastes adoucis, codes couleurs Strava) pour transformer mon rapport en une interface proche d'une application mobile moderne.

> **Toutes les explications techniques, les captures d'Ã©cran du rendu final et le dÃ©tail du design se trouvent dans le dossier dÃ©diÃ© : [Documentation Visualisation](visualisation/README.md)**
> 